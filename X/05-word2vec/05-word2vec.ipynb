{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "- CBOW: Continuous Bow \n",
    "- Skip Gram\n",
    "\n",
    "These both are shallow neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word2Vec**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Example Documents**  \n",
    "- **Document 1**: \"people watch techhub\"  \n",
    "- **Document 2**: \"techhub watch techhub\"  \n",
    "- **Document 3**: \"people write review\"  \n",
    "- **Document 4**: \"techhub write review\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Corpus Definition**  \n",
    "Same corpus as before:  \n",
    "```python\n",
    "corpus = [\n",
    "    \"people watch techhub\",\n",
    "    \"techhub watch techhub\",\n",
    "    \"people write review\",\n",
    "    \"techhub write review\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Word2Vec Overview**  \n",
    "Word2Vec is a **predictive embedding model** that learns:  \n",
    "- **Continuous vector representations** (typically 50-300 dimensions)  \n",
    "- **Semantic relationships** via context (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\")  \n",
    "\n",
    "Two architectures:  \n",
    "1. **Skip-gram**: Predicts context words given a target word.  \n",
    "2. **CBOW (Continuous Bag-of-Words)**: Predicts a target word from context.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Word2Vec Training (Hypothetical Output)**  \n",
    "Assume we train a **2-dimensional** Word2Vec model for illustration:  \n",
    "\n",
    "| Word     | Vector (x, y)    | Notes                  |\n",
    "|----------|------------------|------------------------|\n",
    "| people   | [0.52, 0.85]     | Close to \"write\"       |\n",
    "| watch    | [0.33, 0.94]     | Often co-occurs with \"techhub\" |\n",
    "| techhub  | [0.91, 0.41]     | Central to documents 1,2,4 |\n",
    "| write    | [0.48, 0.88]     | Close to \"people\"      |\n",
    "| review   | [0.76, 0.65]     | Groups with \"write\"    |\n",
    "\n",
    "**Key Observations**:  \n",
    "- Similar words (e.g., \"people\" and \"write\") have **closer vectors**.  \n",
    "- \"techhub\" is distant from \"review\" (rarely co-occur).  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Document Representation**  \n",
    "Unlike BoW/TF-IDF, Word2Vec requires **aggregation** for documents:  \n",
    "\n",
    "#### **Method 1: Average Word Vectors**  \n",
    "- **Doc 1**: `mean([people, watch, techhub]) = [(0.52+0.33+0.91)/3, (0.85+0.94+0.41)/3] ≈ [0.59, 0.73]`  \n",
    "- **Doc 2**: `mean([techhub, watch, techhub]) ≈ [0.72, 0.60]`  \n",
    "- **Doc 3**: `mean([people, write, review]) ≈ [0.59, 0.79]`  \n",
    "- **Doc 4**: `mean([techhub, write, review]) ≈ [0.72, 0.65]`  \n",
    "\n",
    "#### **Method 2: TF-IDF Weighted Average**  \n",
    "Weight vectors by TF-IDF scores for richer semantics.  \n",
    "\n",
    "---\n",
    "\n",
    "### **6. Pros of Word2Vec**  \n",
    "✅ **Captures Semantics**:  \n",
    "   - \"people\" ≈ \"write\" (similar contexts).  \n",
    "✅ **Fixed-Length Vectors**:  \n",
    "   - All words/documents in same space (e.g., 300D).  \n",
    "✅ **Generalizes to Unseen Words**:  \n",
    "   - Similar words get similar vectors (e.g., \"blog\" ≈ \"techhub\").  \n",
    "\n",
    "---\n",
    "\n",
    "### **7. Cons of Word2Vec**  \n",
    "❌ **No Out-of-the-Box Doc Rep**:  \n",
    "   - Requires aggregation (average, TF-IDF) for documents.  \n",
    "❌ **Ignores Word Order**:  \n",
    "   - \"people watch\" ≠ \"watch people\" (no n-grams).  \n",
    "❌ **Data-Hungry**:  \n",
    "   - Needs large corpora for training.  \n",
    "\n",
    "---\n",
    "\n",
    "### **8. Comparison with Other Methods**  \n",
    "\n",
    "| Feature          | TF-IDF            | Word2Vec          |\n",
    "|------------------|-------------------|-------------------|\n",
    "| **Semantics**    | ❌ No             | ✅ Yes            |\n",
    "| **Dimensionality** | Sparse (V)      | Dense (50-300D)   |\n",
    "| **OOV Handling** | ❌ Fails          | ✅ Partial (via similar words) |\n",
    "| **Training**     | None (statistical)| Requires corpus   |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Practical Notes**  \n",
    "- **Pretrained Models**: Use GloVe/FastText for small datasets.  \n",
    "- **Fine-Tuning**: Retrain on domain-specific text (e.g., medical journals).  \n",
    "- **Extensions**:  \n",
    "  - **Doc2Vec**: Direct document embeddings.  \n",
    "  - **BERT**: Captures context-dependent meanings.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "Word2Vec excels at **semantic tasks** (analogy, clustering) but needs **post-processing for documents**. For modern NLP, **contextual embeddings (BERT)** are superior but computationally heavier.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: We'll use the pre-trained weights of word2vec that was trained on Google News corpus containing 3 billion words. This model consists of 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "[('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474), ('sultan', 0.5864824056625366), ('ruler', 0.5797567367553711), ('princes', 0.5646552443504333), ('Prince_Paras', 0.5432944297790527), ('throne', 0.5422105193138123)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download and load the Word2Vec model (~1.6GB)\n",
    "model = api.load(\"word2vec-google-news-300\")  # Same dimensions as the original\n",
    "print(model.most_similar(\"king\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queens', 0.739944338798523),\n",
       " ('princess', 0.7070532441139221),\n",
       " ('king', 0.6510956883430481),\n",
       " ('monarch', 0.6383602023124695),\n",
       " ('very_pampered_McElhatton', 0.6357026696205139),\n",
       " ('Queen', 0.6163407564163208),\n",
       " ('NYC_anglophiles_aflutter', 0.6060680150985718),\n",
       " ('Queen_Consort', 0.5923796892166138),\n",
       " ('princesses', 0.5908074975013733),\n",
       " ('royal', 0.5637185573577881)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3d1f06ec4fd0f335b4bb83d2424fac4fee372c73b45e1b31b74a87e30ed8db2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
