{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Representation Techniques\n",
    "- **ML Based**\n",
    "1. OHE\n",
    "2. Bag of Words\n",
    "3. ngrams\n",
    "4. TFIDF\n",
    "5. Custom Feature\n",
    "\n",
    "- **Deep Learning Based**\n",
    "    - Word2Vec/Vector Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms Used in NLP\n",
    "- Document: Each row in a dataset is called document.\n",
    "- Corpus: Collection of documents(all rows) is called corpus.\n",
    "- Vocabulary: Unique words in corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. One Hot Encoding (OHE)**  \n",
    "\n",
    "---\n",
    "\n",
    "### **1.1 Example Documents**  \n",
    "- **Document 1**: \"people watch techhub\"  \n",
    "- **Document 2**: \"techhub watch techhub\"  \n",
    "- **Document 3**: \"people write review\"  \n",
    "- **Document 4**: \"techhub write review\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **1.2. Corpus Definition**  \n",
    "The **corpus** is the set of all documents:  \n",
    "```\n",
    "{\n",
    "  \"people watch techhub\",\n",
    "  \"techhub watch techhub\",\n",
    "  \"people write review\",\n",
    "  \"techhub write review\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **1.3. Vocabulary Generation**  \n",
    "Unique words across all documents (case-insensitive, no duplicates):  \n",
    "```\n",
    "Vocabulary = {\"people\", \"watch\", \"techhub\", \"write\", \"review\"}\n",
    "```\n",
    "- **Size of Vocabulary (V)**: 5  \n",
    "\n",
    "---\n",
    "\n",
    "### **1.4. One Hot Encoding (OHE) Table**  \n",
    "Each document is represented as a binary vector where:  \n",
    "- `1` = Word present in the document.  \n",
    "- `0` = Word absent.  \n",
    "\n",
    "| Word Index | Word    | Document 1 | Document 2 | Document 3 | Document 4 |\n",
    "|------------|---------|------------|------------|------------|------------|\n",
    "| 0          | people  | 1          | 0          | 1          | 0          |\n",
    "| 1          | watch   | 1          | 1          | 0          | 0          |\n",
    "| 2          | techhub | 1          | 1          | 0          | 1          |\n",
    "| 3          | write   | 0          | 0          | 1          | 1          |\n",
    "| 4          | review  | 0          | 0          | 1          | 1          |\n",
    "\n",
    "**Vector Representations**:  \n",
    "- **Doc 1**: `[1, 1, 1, 0, 0]`  \n",
    "- **Doc 2**: `[0, 1, 1, 0, 0]`  \n",
    "- **Doc 3**: `[1, 0, 0, 1, 1]`  \n",
    "- **Doc 4**: `[0, 0, 1, 1, 1]`  \n",
    "\n",
    "---\n",
    "\n",
    "### **1.5. Pros of One Hot Encoding (OHE)**  \n",
    "✅ **Simplicity**: Easy to implement and interpret.  \n",
    "✅ **Preserves Word Presence**: Clearly marks which words exist in a document.  \n",
    "✅ **Works with Traditional ML Models**: Compatible with algorithms like Naive Bayes, logistic regression.  \n",
    "✅ **No Prior Assumptions**: Treats each word as independent (no semantic relationships).  \n",
    "\n",
    "---\n",
    "\n",
    "### **1.6. Cons of One Hot Encoding (OHE)**  \n",
    "❌ **Sparsity**:  \n",
    "   - Vectors are mostly zeros (high-dimensional but sparse).  \n",
    "   - Inefficient storage/computation (e.g., a vocabulary of 10K words → 10K-dim vectors).  \n",
    "❌ **Fixed Dimensionality**:  \n",
    "   - Vector size depends on vocabulary size.  \n",
    "   - Adding new words requires retraining (no flexibility).  \n",
    "❌ **Out-of-Vocabulary (OOV) Problem**:  \n",
    "   - Fails to encode unseen words (e.g., \"blog\" in a new document).  \n",
    "❌ **No Semantic Meaning**:  \n",
    "   - \"techhub\" and \"review\" are equally distant, ignoring context/relationships.  \n",
    "❌ **Poor for Deep Learning**:  \n",
    "   - Large vocabularies create computational bottlenecks.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1.7. Additional Notes**  \n",
    "- **Computational Inefficiency**: OHE is impractical for large vocabularies (e.g., 1M+ words).  \n",
    "- **Alternative Methods**:  \n",
    "  - **Word Embeddings** (Word2Vec, GloVe): Capture semantic meaning.  \n",
    "  - **TF-IDF**: Weights words by importance.  \n",
    "  - **Subword Tokenization** (e.g., BPE): Handles OOV words.  \n",
    "- **Use Cases**:  \n",
    "  - Best for small datasets or baseline models.  \n",
    "  - Avoid for tasks requiring semantic understanding (e.g., translation, sentiment analysis).  \n",
    "\n",
    "---\n",
    "\n",
    "### **To Concise**  \n",
    "OHE is a **simple but limited** text representation method. While useful for basic tasks, its **sparsity** and **lack of semantics** make it unsuitable for modern NLP applications. Advanced techniques (embeddings, transformers) address these flaws.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Bag of Words (BoW)**  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.1. Example Documents**  \n",
    "- **Document 1**: \"people watch techhub\"  \n",
    "- **Document 2**: \"techhub watch techhub\"  \n",
    "- **Document 3**: \"people write review\"  \n",
    "- **Document 4**: \"techhub write review\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Corpus Definition**  \n",
    "The **corpus** is the set of all documents:  \n",
    "```\n",
    "{\n",
    "  \"people watch techhub\",\n",
    "  \"techhub watch techhub\",\n",
    "  \"people write review\",\n",
    "  \"techhub write review\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3. Vocabulary Generation**  \n",
    "Unique words across all documents (case-insensitive, no duplicates):  \n",
    "```\n",
    "Vocabulary = {\"people\", \"watch\", \"techhub\", \"write\", \"review\"}\n",
    "```\n",
    "- **Size of Vocabulary (V)**: 5  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.4. Bag of Words (BoW) Table**  \n",
    "Each document is represented as a **frequency vector** where:  \n",
    "- Each entry counts the occurrences of a word in the document.  \n",
    "\n",
    "| Word Index | Word    | Document 1 | Document 2 | Document 3 | Document 4 |\n",
    "|------------|---------|------------|------------|------------|------------|\n",
    "| 0          | people  | 1          | 0          | 1          | 0          |\n",
    "| 1          | watch   | 1          | 1          | 0          | 0          |\n",
    "| 2          | techhub | 1          | 2          | 0          | 1          |\n",
    "| 3          | write   | 0          | 0          | 1          | 1          |\n",
    "| 4          | review  | 0          | 0          | 1          | 1          |\n",
    "\n",
    "**Vector Representations**:  \n",
    "- **Doc 1**: `[1, 1, 1, 0, 0]`  \n",
    "- **Doc 2**: `[0, 1, 2, 0, 0]`  \n",
    "- **Doc 3**: `[1, 0, 0, 1, 1]`  \n",
    "- **Doc 4**: `[0, 0, 1, 1, 1]`  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.5. Pros of Bag of Words (BoW)**  \n",
    "✅ **Simple & Intuitive**: Easy to implement and understand.  \n",
    "✅ **Preserves Word Frequency**: Captures term importance (unlike OHE).  \n",
    "✅ **Works with Traditional ML Models**: Compatible with Naive Bayes, logistic regression, etc.  \n",
    "✅ **No Semantic Assumptions**: Treats words as independent (no context needed).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.6. Cons of Bag of Words (BoW)**  \n",
    "❌ **Sparsity**:  \n",
    "   - High-dimensional vectors with many zeros (inefficient storage).  \n",
    "❌ **Fixed Vocabulary Size**:  \n",
    "   - Adding new words requires retraining.  \n",
    "❌ **Out-of-Vocabulary (OOV) Problem**:  \n",
    "   - Cannot handle unseen words (e.g., \"blog\" in a new document).  \n",
    "❌ **No Semantic or Order Information**:  \n",
    "   - \"techhub watch\" and \"watch techhub\" are identical in BoW.  \n",
    "   - Ignores word relationships (e.g., synonyms, antonyms).  \n",
    "❌ **Dominance of Common Words**:  \n",
    "   - Frequent words (e.g., \"the\", \"and\") may overshadow rare but meaningful terms.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.7. Additional Notes**  \n",
    "- **Improvements Over OHE**:  \n",
    "  - BoW captures **term frequency**, while OHE only tracks presence/absence.  \n",
    "- **Common Enhancements**:  \n",
    "  - **TF-IDF**: Weights words by importance (reduces bias from common words).  \n",
    "  - **N-grams**: Captures word sequences (e.g., \"techhub watch\" → bigram).  \n",
    "- **Use Cases**:  \n",
    "  - Best for **small datasets** or **baseline models**.  \n",
    "  - Avoid for tasks needing **context** (e.g., machine translation).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with One Hot Encoding (OHE)**  \n",
    "| Feature          | OHE                          | BoW                          |\n",
    "|------------------|------------------------------|------------------------------|\n",
    "| **Representation** | Binary (0/1)                 | Integer counts               |\n",
    "| **Semantics**     | No word importance           | Captures frequency           |\n",
    "| **Sparsity**      | Extreme (mostly 0s)          | High (but less than OHE)     |\n",
    "| **OOV Handling**  | Fails                       | Fails                       |\n",
    "| **Use Case**      | Basic text classification   | Slightly richer frequency modeling |  \n",
    "\n",
    "---\n",
    "\n",
    "### **To Concise**  \n",
    "BoW is a **frequency-based upgrade** to OHE but still suffers from **sparsity** and **lack of semantics**. Modern NLP prefers **embeddings (Word2Vec, BERT)** or **TF-IDF** for better performance.  \n",
    "\n",
    "### Tip\n",
    "If you draw the vectors of both of these docs they will look similar having a little diff of `not` only.\n",
    "- **This is a very good movie**\n",
    "- **This is not a very good movie**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch techhub</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>techhub watch techhub</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write review</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>techhub write review</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch techhub       1\n",
       "1  techhub watch techhub       1\n",
       "2    people write review       0\n",
       "3   techhub write review       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\":[\"people watch techhub\",\"techhub watch techhub\",\"people write review\",\"techhub write review\"],\n",
    "        \"output\":[1,1,0,0]\n",
    "    }\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 0, 'watch': 3, 'techhub': 2, 'write': 4, 'review': 1}\n"
     ]
    }
   ],
   "source": [
    "bow = cv.fit_transform(df['text'])\n",
    "print(cv.vocabulary_)\n",
    "# Here 0,1,2,3, and 4 are indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0]]\n",
      "[[0 0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray()) # people, index:0, watch:3, techhub:2\n",
    "print(bow[1].toarray()) # techhub:2, watch:3, techhub:2 ---> techhub, index:2, counts:2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['techhub watch and write review']).toarray()\n",
    "# OOV: and is handled in bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3: Bag of N-Grams**  \n",
    "\n",
    "---\n",
    "\n",
    "### **3.1. Example Documents**  \n",
    "- **Document 1**: \"people watch techhub\"  \n",
    "- **Document 2**: \"techhub watch techhub\"  \n",
    "- **Document 3**: \"people write review\"  \n",
    "- **Document 4**: \"techhub write review\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **3.2. Corpus Definition**  \n",
    "The **corpus** is the set of all documents:  \n",
    "```\n",
    "{\n",
    "  \"people watch techhub\",\n",
    "  \"techhub watch techhub\",\n",
    "  \"people write review\",\n",
    "  \"techhub write review\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3. Vocabulary Generation (Unigrams + Bigrams)**  \n",
    "Unique **1-grams (unigrams)** and **2-grams (bigrams)** across all documents:  \n",
    "\n",
    "#### **Unigrams**:  \n",
    "```\n",
    "{\"people\", \"watch\", \"techhub\", \"write\", \"review\"}\n",
    "```  \n",
    "\n",
    "#### **Bigrams**:  \n",
    "```\n",
    "{\"people watch\", \"watch techhub\", \"techhub watch\", \"write review\", \"techhub write\"}\n",
    "```  \n",
    "\n",
    "#### **Combined Vocabulary (Unigrams + Bigrams)**:  \n",
    "```\n",
    "{\n",
    "  \"people\", \"watch\", \"techhub\", \"write\", \"review\",  \n",
    "  \"people watch\", \"watch techhub\", \"techhub watch\", \"write review\", \"techhub write\"\n",
    "}\n",
    "```  \n",
    "- **Size of Vocabulary (V)**: 10  \n",
    "\n",
    "---\n",
    "\n",
    "### **3.4. Bag of N-Grams Table**  \n",
    "Each document is represented as a **frequency vector** of unigrams + bigrams.  \n",
    "\n",
    "| Index | N-Gram          | Document 1 | Document 2 | Document 3 | Document 4 |\n",
    "|-------|-----------------|------------|------------|------------|------------|\n",
    "| 0     | people          | 1          | 0          | 1          | 0          |\n",
    "| 1     | watch           | 1          | 1          | 0          | 0          |\n",
    "| 2     | techhub         | 1          | 2          | 0          | 1          |\n",
    "| 3     | write           | 0          | 0          | 1          | 1          |\n",
    "| 4     | review          | 0          | 0          | 1          | 1          |\n",
    "| 5     | people watch    | 1          | 0          | 0          | 0          |\n",
    "| 6     | watch techhub   | 1          | 1          | 0          | 0          |\n",
    "| 7     | techhub watch   | 0          | 1          | 0          | 0          |\n",
    "| 8     | write review    | 0          | 0          | 1          | 1          |\n",
    "| 9     | techhub write   | 0          | 0          | 0          | 1          |\n",
    "\n",
    "**Vector Representations**:  \n",
    "- **Doc 1**: `[1, 1, 1, 0, 0, 1, 1, 0, 0, 0]`  \n",
    "- **Doc 2**: `[0, 1, 2, 0, 0, 0, 1, 1, 0, 0]`  \n",
    "- **Doc 3**: `[1, 0, 0, 1, 1, 0, 0, 0, 1, 0]`  \n",
    "- **Doc 4**: `[0, 0, 1, 1, 1, 0, 0, 0, 1, 1]`  \n",
    "\n",
    "---\n",
    "\n",
    "### **3.5. Pros of Bag of N-Grams**  \n",
    "✅ **Captures Local Word Order**:  \n",
    "   - Bigrams/trigrams preserve phrases (e.g., \"techhub watch\" ≠ \"watch techhub\").  \n",
    "✅ **Better Context than BoW**:  \n",
    "   - \"not good\" vs. \"good\" are distinguished.  \n",
    "✅ **Works with Traditional ML**:  \n",
    "   - Compatible with classifiers like SVM, Naive Bayes.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3.6. Cons of Bag of N-Grams**  \n",
    "❌ **Higher Dimensionality**:  \n",
    "   - Vocabulary grows exponentially (e.g., 10K words → 100M possible bigrams).  \n",
    "❌ **Still No Semantics**:  \n",
    "   - \"happy joy\" and \"joy happy\" are treated as different.  \n",
    "❌ **Sparsity Worsens**:  \n",
    "   - More zeros in vectors than BoW.  \n",
    "❌ **OOV Problem Persists**:  \n",
    "   - New n-grams (e.g., \"techhub review\") are ignored.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3.7. Additional Notes**  \n",
    "- **Trade-off**:  \n",
    "  - Higher *n* (e.g., trigrams) captures more context but increases sparsity.  \n",
    "- **Common Use Cases**:  \n",
    "  - Sentiment analysis (e.g., \"not bad\" vs. \"bad\").  \n",
    "  - Short-text classification (e.g., tweets).  \n",
    "- **Alternatives**:  \n",
    "  - **TF-IDF Weighting**: Reduces bias from frequent n-grams.  \n",
    "  - **Word Embeddings**: Better for semantic tasks (e.g., Word2Vec).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with BoW and OHE**  \n",
    "| Feature          | OHE               | BoW               | Bag of N-Grams          |\n",
    "|------------------|-------------------|-------------------|-------------------------|\n",
    "| **Represents**   | Binary presence   | Word counts       | N-gram counts           |\n",
    "| **Word Order**   | ❌ No             | ❌ No             | ✅ Yes (local only)     |\n",
    "| **Dimensionality**| Low (V)          | Low (V)          | High (V + Vⁿ)           |\n",
    "| **Use Case**     | Baseline models  | Frequency analysis| Phrase-sensitive tasks  |\n",
    "\n",
    "---\n",
    "\n",
    "### **To Conise**  \n",
    "Bag of N-Grams improves over BoW by **capturing phrases** but suffers from **high sparsity**. For advanced NLP, **embeddings (Word2Vec, BERT)** or **TF-IDF-weighted n-grams** are preferred.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,1)) # default is (1,1) which means the bag of words is the special case of ngrams.\n",
    "# bag of words: unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch': 0,\n",
       " 'watch techhub': 4,\n",
       " 'techhub watch': 2,\n",
       " 'people write': 1,\n",
       " 'write review': 5,\n",
       " 'techhub write': 3}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2)) # bigram\n",
    "bigram = cv.fit_transform(df['text'])\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 0,\n",
       " 'watch': 7,\n",
       " 'techhub': 4,\n",
       " 'people watch': 1,\n",
       " 'watch techhub': 8,\n",
       " 'techhub watch': 5,\n",
       " 'write': 9,\n",
       " 'review': 3,\n",
       " 'people write': 2,\n",
       " 'write review': 10,\n",
       " 'techhub write': 6}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2)) # unigram + bigram\n",
    "bigram = cv.fit_transform(df['text'])\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF (Term Frequency-Inverse Document Frequency)**  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Example Documents**  \n",
    "- **Document 1 (D1)**: \"people watch techhub\"  \n",
    "- **Document 2 (D2)**: \"techhub watch techhub\"  \n",
    "- **Document 3 (D3)**: \"people write review\"  \n",
    "- **Document 4 (D4)**: \"techhub write review\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Corpus Definition**  \n",
    "Same as before:  \n",
    "```python\n",
    "corpus = [\n",
    "    \"people watch techhub\",\n",
    "    \"techhub watch techhub\",\n",
    "    \"people write review\",\n",
    "    \"techhub write review\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Vocabulary Generation**  \n",
    "Unique words across all documents:  \n",
    "```python\n",
    "vocabulary = [\"people\", \"watch\", \"techhub\", \"write\", \"review\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. TF-IDF Calculation**  \n",
    "\n",
    "#### **Step 1: Compute Term Frequency (TF)**  \n",
    "`TF(t, d) = (Number of times term t appears in document d) / (Total terms in d)`  \n",
    "\n",
    "| Term     | D1   | D2   | D3   | D4   |\n",
    "|----------|------|------|------|------|\n",
    "| people   | 1/3  | 0    | 1/3  | 0    |\n",
    "| watch    | 1/3  | 1/3  | 0    | 0    |\n",
    "| techhub  | 1/3  | 2/3  | 0    | 1/3  |\n",
    "| write    | 0    | 0    | 1/3  | 1/3  |\n",
    "| review   | 0    | 0    | 1/3  | 1/3  |\n",
    "\n",
    "#### **Step 2: Compute Inverse Document Frequency (IDF)**  \n",
    "`IDF(t) = log(Total documents / Number of documents containing t) + 1`  \n",
    "\n",
    "| Term     | Doc Freq | IDF               |\n",
    "|----------|----------|-------------------|\n",
    "| people   | 2        | log(4/2) + 1 ≈ 1.693 |\n",
    "| watch    | 2        | log(4/2) + 1 ≈ 1.693 |\n",
    "| techhub  | 3        | log(4/3) + 1 ≈ 1.287 |\n",
    "| write    | 2        | log(4/2) + 1 ≈ 1.693 |\n",
    "| review   | 2        | log(4/2) + 1 ≈ 1.693 |\n",
    "\n",
    "#### **Step 3: Compute TF-IDF = TF × IDF**  \n",
    "\n",
    "| Term     | D1        | D2        | D3        | D4        |\n",
    "|----------|-----------|-----------|-----------|-----------|\n",
    "| people   | 0.564     | 0         | 0.564     | 0         |\n",
    "| watch    | 0.564     | 0.564     | 0         | 0         |\n",
    "| techhub  | 0.429     | 0.858     | 0         | 0.429     |\n",
    "| write    | 0         | 0         | 0.564     | 0.564     |\n",
    "| review   | 0         | 0         | 0.564     | 0.564     |\n",
    "\n",
    "**Final TF-IDF Vectors**:  \n",
    "- **D1**: `[0.564, 0.564, 0.429, 0, 0]`  \n",
    "- **D2**: `[0, 0.564, 0.858, 0, 0]`  \n",
    "- **D3**: `[0.564, 0, 0, 0.564, 0.564]`  \n",
    "- **D4**: `[0, 0, 0.429, 0.564, 0.564]`  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Pros of TF-IDF**  \n",
    "✅ **Weighted Importance**:  \n",
    "   - Rare terms get higher weights (e.g., \"review\" > \"techhub\").  \n",
    "✅ **Reduces Dominance of Common Words**:  \n",
    "   - Frequent words (e.g., \"techhub\") are downweighted.  \n",
    "✅ **Works with Sparse Data**:  \n",
    "   - Better than raw counts for ML models.  \n",
    "✅ **No Semantic Assumptions**:  \n",
    "   - Pure statistical weighting.  \n",
    "\n",
    "---\n",
    "\n",
    "### **6. Cons of TF-IDF**  \n",
    "❌ **Still No Word Order**:  \n",
    "   - \"techhub watch\" ≠ \"watch techhub\".  \n",
    "❌ **OOV Problem Persists**:  \n",
    "   - Unseen terms get zero weight.  \n",
    "❌ **Dimensionality Issues**:  \n",
    "   - Large vocabularies → high-dimensional vectors.  \n",
    "❌ **Manual Tuning Needed**:  \n",
    "   - IDF smoothing (e.g., `+1`) requires experimentation.  \n",
    "\n",
    "---\n",
    "\n",
    "### **7. Additional Notes**  \n",
    "- **N-Grams + TF-IDF**:  \n",
    "  - Apply TF-IDF to bigrams/trigrams for phrase-aware weighting.  \n",
    "- **Normalization**:  \n",
    "  - Often, vectors are L2-normalized for cosine similarity.  \n",
    "- **Use Cases**:  \n",
    "  - Search engines, document clustering, and preprocessing for classifiers.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with Other Methods**  \n",
    "\n",
    "| Feature          | OHE               | BoW               | TF-IDF             |\n",
    "|------------------|-------------------|-------------------|--------------------|\n",
    "| **Weighting**    | Binary (0/1)      | Raw counts        | Term importance    |\n",
    "| **Word Order**   | ❌ No             | ❌ No             | ❌ No              |\n",
    "| **Handles Common Words** | ❌ No      | ❌ No             | ✅ Yes             |\n",
    "| **Dimensionality** | Fixed (V)       | Fixed (V)         | Fixed (V)          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "TF-IDF improves on BoW by **emphasizing rare, informative terms** but still ignores semantics. For modern NLP, **TF-IDF + n-grams** or **embeddings (Word2Vec, BERT)** are stronger choices.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     people    review   techhub     watch     write  output\n",
      "0  0.613667  0.000000  0.496816  0.613667  0.000000       1\n",
      "1  0.000000  0.000000  0.850816  0.525464  0.000000       1\n",
      "2  0.577350  0.577350  0.000000  0.000000  0.577350       0\n",
      "3  0.000000  0.613667  0.496816  0.000000  0.613667       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\"people watch techhub\", \"techhub watch techhub\", \"people write review\", \"techhub write review\"],\n",
    "        \"output\": [1, 1, 0, 0]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data into the TF-IDF matrix\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Convert the result to a DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Add the output column to the DataFrame\n",
    "tfidf_df['output'] = df['output']\n",
    "\n",
    "# Display the resulting DataFrame with TF-IDF values\n",
    "print(tfidf_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3d1f06ec4fd0f335b4bb83d2424fac4fee372c73b45e1b31b74a87e30ed8db2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
